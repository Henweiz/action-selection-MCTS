{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7156c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mctx\n",
    "!pip install jumanji\n",
    "!pip install flashbax\n",
    "!pip install dm-haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a85304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver.py\n",
    "from typing import Callable, Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def bisection_method(\n",
    "        f: Callable[[jax.Array, dict[str, Any]], jax.Array],\n",
    "        n: int,\n",
    "        max_steps: int,\n",
    "        step_size: float = 0.0\n",
    ") -> Callable[..., jax.Array]:\n",
    "    \"\"\"Compile a bisection method.\n",
    "\n",
    "    Converges to the root (if it exists) with O(n * sqrt(max_steps))\n",
    "\n",
    "    Only supports 1-Dimensional search.\n",
    "    \"\"\"\n",
    "    batch_fun = jax.vmap(f, in_axes=(0, None))\n",
    "\n",
    "    def body(\n",
    "            carry: tuple[tuple[jax.Array, jax.Array], dict],\n",
    "            x: None = None\n",
    "    ) -> tuple[\n",
    "        tuple[tuple[jax.Array, jax.Array], dict],\n",
    "        jax.Array | float\n",
    "    ]:\n",
    "        prev_bounds, kwargs = carry\n",
    "\n",
    "        grid = jnp.linspace(*prev_bounds, 2 * n + 1)\n",
    "        step = grid[1] - grid[0]\n",
    "        values = batch_fun(grid, kwargs).squeeze()\n",
    "        values = jnp.nan_to_num(values, nan=1e32, neginf=-1e10, posinf=1e10)\n",
    "\n",
    "        # Transform values to make the function's roots an attractor.\n",
    "        # Then argmax/ argmin can find the best bounds without array reshaping.\n",
    "        best_positive = grid.at[jnp.argmax(1.0 / (values + 1e-32))].get()\n",
    "        best_negative = grid.at[jnp.argmin(1.0 / (values - 1e-32))].get()\n",
    "\n",
    "        # Construct new-bounds closest to the function root with some slack.\n",
    "        new_bounds = (best_negative - step * step_size,\n",
    "                      best_positive + step * step_size)\n",
    "\n",
    "        return (new_bounds, kwargs), sum(new_bounds) / 2.0\n",
    "\n",
    "    def run(\n",
    "            bounds: tuple[jax.typing.ArrayLike, jax.typing.ArrayLike],\n",
    "            kwargs: dict[str, Any]\n",
    "    ) -> jax.Array:\n",
    "        _, best = jax.lax.scan(\n",
    "            body, (bounds, kwargs), xs=None, length=max_steps\n",
    "        )\n",
    "        return best[-1]\n",
    "\n",
    "    return run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168fccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_2048.py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from jax import grad, jit\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "\n",
    "class PolicyValueNetwork_2048(nn.Module):\n",
    "    num_actions: int\n",
    "    num_channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Conv Layers + MLP Layer.\n",
    "        k_size = (2, 2)\n",
    "        x = nn.Conv(features=self.num_channels, kernel_size=k_size)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = jnp.reshape(x, (x.shape[0], -1))  # Flatten\n",
    "\n",
    "        # Policy Layers.\n",
    "        actions = nn.Dense(128)(x) # TODO try value norm\n",
    "        actions = nn.leaky_relu(actions)\n",
    "        actions = nn.Dense(128)(actions)\n",
    "        actions = nn.leaky_relu(actions)\n",
    "        actions = nn.Dense(self.num_actions)(actions)\n",
    "        actions = nn.softmax(actions)\n",
    "\n",
    "        # Value Layers\n",
    "        value = nn.Dense(256)(x)\n",
    "        value = nn.leaky_relu(value)\n",
    "        value = nn.Dense(256)(value)\n",
    "        value = nn.leaky_relu(value)\n",
    "        value = nn.Dense(1)(value)\n",
    "\n",
    "        return actions, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadb3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_knapsack.py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import chex\n",
    "import haiku as hk\n",
    "from flax.linen import compact\n",
    "from jumanji.training.networks.knapsack.actor_critic import (\n",
    "    make_knapsack_masks,\n",
    "    make_knapsack_query,\n",
    "    KnapsackTorso,\n",
    ")\n",
    "from jumanji.environments.packing.knapsack.types import Observation\n",
    "from typing import Tuple\n",
    "\n",
    "config = {\n",
    "    \"transformer_num_blocks\": 6,\n",
    "    \"transformer_num_heads\": 8,\n",
    "    \"transformer_key_size\": 16,\n",
    "    \"transformer_mlp_units\": [512],\n",
    "}\n",
    "\n",
    "def value_fn(observation: Observation) -> chex.Array:\n",
    "    torso = KnapsackTorso(\n",
    "        transformer_num_blocks=config[\"transformer_num_blocks\"],\n",
    "        transformer_num_heads=config[\"transformer_num_heads\"],\n",
    "        transformer_key_size=config[\"transformer_key_size\"],\n",
    "        transformer_mlp_units=config[\"transformer_mlp_units\"],\n",
    "        name=\"torso\",\n",
    "    )\n",
    "    self_attention_mask, cross_attention_mask = make_knapsack_masks(observation)\n",
    "    items_features = jnp.concatenate(\n",
    "        [observation.weights[..., None], observation.values[..., None]], axis=-1\n",
    "    )\n",
    "    embeddings = torso(items_features, self_attention_mask)\n",
    "    query = make_knapsack_query(observation, embeddings)\n",
    "    cross_attention_block = hk.MultiHeadAttention(\n",
    "        num_heads=config[\"transformer_num_heads\"],\n",
    "        key_size=config[\"transformer_key_size\"],\n",
    "        w_init=hk.initializers.VarianceScaling(1.0),\n",
    "        name=\"cross_attention_block\",\n",
    "    )\n",
    "    cross_attention = cross_attention_block(\n",
    "        query=query,\n",
    "        value=embeddings,\n",
    "        key=embeddings,\n",
    "        mask=cross_attention_mask,\n",
    "    ).squeeze(axis=-2)\n",
    "    values = jnp.einsum(\"...Tk,...k->...T\", embeddings, cross_attention)\n",
    "    values = values / jnp.sqrt(cross_attention_block.model_size)\n",
    "    value = values.sum(axis=-1, where=cross_attention_mask.squeeze(axis=(-2, -3)))\n",
    "    return value\n",
    "\n",
    "\n",
    "def policy_fn(observation: Observation) -> chex.Array:\n",
    "    torso = KnapsackTorso(\n",
    "        transformer_num_blocks=config[\"transformer_num_blocks\"],\n",
    "        transformer_num_heads=config[\"transformer_num_heads\"],\n",
    "        transformer_key_size=config[\"transformer_key_size\"],\n",
    "        transformer_mlp_units=config[\"transformer_mlp_units\"],\n",
    "        name=\"torso\",\n",
    "    )\n",
    "    self_attention_mask, cross_attention_mask = make_knapsack_masks(observation)\n",
    "    items_features = jnp.concatenate(\n",
    "        [observation.weights[..., None], observation.values[..., None]], axis=-1\n",
    "    )\n",
    "    embeddings = torso(items_features, self_attention_mask)\n",
    "    query = make_knapsack_query(observation, embeddings)\n",
    "    cross_attention_block = hk.MultiHeadAttention(\n",
    "        num_heads=config[\"transformer_num_heads\"],\n",
    "        key_size=config[\"transformer_key_size\"],\n",
    "        w_init=hk.initializers.VarianceScaling(1.0),\n",
    "        name=\"cross_attention_block\",\n",
    "    )\n",
    "    cross_attention = cross_attention_block(\n",
    "        query=query,\n",
    "        value=embeddings,\n",
    "        key=embeddings,\n",
    "        mask=cross_attention_mask,\n",
    "    ).squeeze(axis=-2)\n",
    "    logits = jnp.einsum(\"...Tk,...k->...T\", embeddings, cross_attention)\n",
    "    logits = logits / jnp.sqrt(cross_attention_block.model_size)\n",
    "    logits = 10 * jnp.tanh(logits)  # clip to [-10,10]\n",
    "    logits = jnp.where(observation.action_mask, logits, jnp.finfo(jnp.float32).min)\n",
    "    actions = jax.nn.softmax(logits)\n",
    "    return actions\n",
    "\n",
    "\n",
    "def forward_fn(inputs) -> Tuple[chex.Array, chex.Array]:\n",
    "    weights = inputs[:, 0, :].astype(jnp.float32)\n",
    "    values = inputs[:, 1, :].astype(jnp.float32)\n",
    "    packed_items = inputs[:, 2, :].astype(jnp.bool)\n",
    "    action_mask = inputs[:, 3, :].astype(jnp.bool)\n",
    "    observation = Observation(weights, values, packed_items, action_mask)\n",
    "    return policy_fn(observation), value_fn(observation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f8c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from jax import grad, jit\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from jax import random\n",
    "\n",
    "\n",
    "class CNNPolicyNetwork(nn.Module):\n",
    "    \"\"\"A simple policy network that outputs a probability distribution over actions.\"\"\"\n",
    "\n",
    "    num_actions: int # Number of possible actions\n",
    "    num_channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        #x = jnp.reshape(x, (x.shape[0], -1)) #flatten, do not that we get errors when we do not input batches\n",
    "        k_size = (3, 3)\n",
    "\n",
    "        x = nn.Conv(features=self.num_channels, kernel_size=k_size)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Conv(features=self.num_channels, kernel_size=k_size)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = jnp.reshape(x, (x.shape[0], -1))  # Flatten\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(64)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(self.num_actions)(x)\n",
    "        x = nn.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the Value Network\n",
    "class CNNValueNetwork(nn.Module):\n",
    "    \"\"\"A simple value network.\"\"\"\n",
    "    #num_outputs: int = 1\n",
    "    num_channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # key = random.PRNGKey(758493)\n",
    "        # x = random.uniform(key, shape=x.shape)\n",
    "        k_size = (3, 3)\n",
    "\n",
    "        #x = jnp.reshape(x, (x.shape[0], -1)) # flatten\n",
    "        x = nn.Conv(features=self.num_channels, kernel_size=k_size)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Conv(features=self.num_channels, kernel_size=k_size)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = jnp.reshape(x, (x.shape[0], -1))  # Flatten\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(128)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_snake.py\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "    num_actions: int\n",
    "    num_channels: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Conv Layers + MLP Layer.\n",
    "        k_size = (2, 2)\n",
    "        x = nn.Conv(features=self.num_channels, kernel_size=k_size, strides=(2, 2))(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.Conv(features=self.num_channels, kernel_size=k_size)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = jnp.reshape(x, (x.shape[0], -1))  # Flatten\n",
    "\n",
    "\n",
    "        # Policy Layers.\n",
    "        actions = nn.Dense(64)(x)\n",
    "        actions = nn.leaky_relu(actions)\n",
    "        actions = nn.Dense(64)(actions)\n",
    "        actions = nn.leaky_relu(actions)\n",
    "        actions = nn.Dense(self.num_actions)(actions)\n",
    "        actions = nn.softmax(actions)\n",
    "\n",
    "        # Value Layers\n",
    "        value = nn.Dense(128)(x)\n",
    "        value = nn.leaky_relu(value)\n",
    "        value = nn.Dense(128)(value)\n",
    "        value = nn.leaky_relu(value)\n",
    "        value = nn.Dense(1)(value)\n",
    "\n",
    "        return actions, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb_logging.py\n",
    "import wandb\n",
    "import jax.numpy as jnp\n",
    "\n",
    "env_short_names = {\n",
    " \"Game2048-v1\": \"2048\",\n",
    "    \"Knapsack-v1\": \"Knapsack\",\n",
    "    \"Maze-v0\": \"Maze\",\n",
    "    \"Snake-v1\": \"Snake\",\n",
    "}\n",
    "\n",
    "def init_wandb(params):\n",
    "    if params[\"run_in_kaggle\"]:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        wandb_api = user_secrets.get_secret(\"wandb_api\")\n",
    "    else:\n",
    "        import os\n",
    "        wandb_api = os.environ['WANDB_API_KEY']\n",
    "\n",
    "    relevant_params = {k: v for k, v in params.items() if k not in\n",
    "                      [\"maze_size\", \"agent\", \"num_actions\", \"obs_spec\", \"run_in_kaggle\", \"logging\", \"buffer_max_length\", \"buffer_min_length\"]}\n",
    "\n",
    "    wandb.login(key=wandb_api)\n",
    "    wandb.init(\n",
    "        project=\"action-selection-mcts\",\n",
    "        name=f\"{env_short_names[params['env_name']]}_{params['policy']}_sim{params['num_simulations']}_seed{params['seed']}\",\n",
    "        config=relevant_params)\n",
    "\n",
    "\n",
    "def log_rewards(reward, loss, episode, params):\n",
    "    wandb.log(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting.py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "def plot_losses(all_results_array):\n",
    "\n",
    "    # Assuming all_results_array is a numpy array\n",
    "    all_results_array = np.array(all_results_array)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the Total Return on the left y-axis\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    if all_results_array.shape[1] == 3: #Single, combined loss\n",
    "        ax1.plot(all_results_array[:, 2], label=\"Loss\", color='b')\n",
    "        ax1.set_ylabel('Loss', color='b')\n",
    "    else: #Separate value and policy loss\n",
    "        ax1.set_ylabel('Value Loss', color='b')\n",
    "        ax1.plot(all_results_array[:, 2], label=\"Value Loss\", color='b')\n",
    "\n",
    "        # Create a second y-axis sharing the same x-axis\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(all_results_array[:, 3], label=\"Policy Loss\", color='r')\n",
    "        ax2.set_ylabel('Policy Loss', color='r')\n",
    "        ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "    # Adding legends to both axes\n",
    "    fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "    plt.show()\n",
    "    if wandb.run is not None:\n",
    "        plot = wandb.Image(fig)\n",
    "        wandb.log({\"Loss\": plot})\n",
    "        plt.clf()\n",
    "\n",
    "def plot_rewards(all_results_array):\n",
    "    # Assuming all_results_array is a numpy array\n",
    "    all_results_array = np.array(all_results_array)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the Total Return on the left y-axis\n",
    "    ax1.plot(all_results_array[:, 0], label=\"Total Return\", color='b')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Return', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Create a second y-axis sharing the same x-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(all_results_array[:, 1], label=\"Max Return\", color='r')\n",
    "    ax2.set_ylabel('Max Return', color='r')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "    # Adding legends to both axes\n",
    "    fig.legend(loc=\"upper right\", bbox_to_anchor=(1, 1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "    plt.show()\n",
    "    if wandb.run is not None:\n",
    "        plot = wandb.Image(fig)\n",
    "        wandb.log({\"Returns\": plot})\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51791d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface.py\n",
    "from typing import Any\n",
    "\n",
    "import abc\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.typing as jxt\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PolicyObjective(abc.ABC):\n",
    "    \"\"\"Base class for implementing proposal distributions for control.\n",
    "\n",
    "    The proposal distribution solves a constrained program,\n",
    "        max_q <Q, q>\n",
    "        s.t.,\n",
    "        D(q, pi) < epsilon\n",
    "        int q(a) da = 1\n",
    "\n",
    "    For the hard-constraint, one needs to specify epsilon to __call__.\n",
    "    For the soft-constraint solution, one can also specify the Lagrange\n",
    "    multiplier directly.\n",
    "\n",
    "    Only discrete spaces are supported for a very good reason. For\n",
    "    compatibility with continuous spaces one needs to sample from `pi`\n",
    "    directly, and then passing a uniform distribution to __call__.\n",
    "    \"\"\"\n",
    "    solver: Any | None = None\n",
    "    logits: bool = False\n",
    "\n",
    "    # Tolerance constants for validating solutions.\n",
    "    _norm_tolerance: float = 0.001\n",
    "    _epsilon_ltol: float = 0.001\n",
    "    _epsilon_rtol: float = 0.01\n",
    "\n",
    "    def set_solver(self, solver):\n",
    "        self.solver = solver\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_args(\n",
    "            inv_beta: jxt.ArrayLike | None,\n",
    "            epsilon: jxt.ArrayLike | None,\n",
    "            raise_ambiguity: bool = False\n",
    "    ):\n",
    "        if (epsilon is None) and (inv_beta is None):\n",
    "            raise ValueError(\"Both `epsilon` and `inv_beta` cannot be None!\")\n",
    "\n",
    "        if raise_ambiguity:\n",
    "            if (epsilon is not None) and (inv_beta is not None):\n",
    "                raise ValueError(\n",
    "                    \"Ambiguity Error. Values given for both \"\n",
    "                    \"`epsilon` and `inv_beta`! \"\n",
    "                )\n",
    "\n",
    "    @staticmethod\n",
    "    def epsilon_greedy(\n",
    "            q: jax.Array,\n",
    "            eps: jax.typing.ArrayLike = 0.0\n",
    "    ) -> jax.Array:\n",
    "        greedy = (q == q.max())\n",
    "        return (greedy / greedy.sum()) * (1 - eps) + eps / q.size\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def trust_region_upperbound(\n",
    "            self, q: jax.Array, pi: jax.Array\n",
    "    ) -> jax.Array:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def lagrangian(self, *args, **kwargs) -> tuple:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abc.abstractmethod\n",
    "    def divergence(\n",
    "            q_star: jax.Array,\n",
    "            pi: jax.Array\n",
    "    ) -> jxt.ArrayLike:\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __call__(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            pi: jax.Array,\n",
    "            *,\n",
    "            epsilon: jxt.ArrayLike | None = None,\n",
    "            inv_beta: jxt.ArrayLike | None = None\n",
    "    ) -> jax.Array:\n",
    "        # Compute Lagrangian solution w.r.t. q\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AnalyticObjective(PolicyObjective, abc.ABC):\n",
    "    \"\"\"Base class for objectives that are fully analytical.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, logits: bool = False):\n",
    "        super().__init__(solver=object(), logits=logits)\n",
    "\n",
    "    def lagrangian(self, *args, **kwargs) -> tuple:\n",
    "        return 0,  # Solution is fully analytical\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NumericalNormalizerObjective(PolicyObjective, abc.ABC):\n",
    "    \"\"\"Base class for objectives without analytical trust-region constraint.\n",
    "\n",
    "    Provides solver utility to estimate the optimal Lagrange multiplier.\n",
    "    \"\"\"\n",
    "    num_init: int = 30\n",
    "    recursive_steps: int = 10\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.solver is None:\n",
    "            self.solver = bisection_method(\n",
    "                lambda x, k: self._objective(x, **k),\n",
    "                self.num_init, self.recursive_steps\n",
    "            )\n",
    "\n",
    "    def _objective(self, x, **kwargs) -> jax.Array:\n",
    "        return self.lagrangian(x, **kwargs)[0]\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_search_bounds(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            log_pi: jax.Array,\n",
    "            *,\n",
    "            inv_beta: jax.Array | None = None,\n",
    "            epsilon: jax.Array | None = None\n",
    "    ) -> tuple[jax.Array, jax.Array]:\n",
    "        \"\"\"Get informative bounds for the normalizer in the true search-space.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def solve_normalizer(self, *args, **kwargs) -> jxt.ArrayLike:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NumericalTrustRegionObjective(PolicyObjective, abc.ABC):\n",
    "    \"\"\"Base class for objectives without analytical trust-region constraint.\n",
    "\n",
    "    Provides solver utility to estimate the optimal Lagrange multiplier.\n",
    "    \"\"\"\n",
    "    num_init: int = 30\n",
    "    recursive_steps: int = 10\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.solver is None:\n",
    "            self.solver = bisection_method(\n",
    "                lambda x, k: self._objective(x, **k),\n",
    "                self.num_init, self.recursive_steps\n",
    "            )\n",
    "\n",
    "    def _objective(self, x, **kwargs) -> jax.Array:\n",
    "        return self.lagrangian(x, **kwargs)[0]\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def solve_trust_region(self, *args, **kwargs) -> jxt.ArrayLike:\n",
    "        pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class NumericalNormalizerAndTrustRegionObjective(PolicyObjective, abc.ABC):\n",
    "    \"\"\"Base class for objectives without analytical trust-region constraint.\n",
    "\n",
    "    Provides solver utility to estimate the optimal Lagrange multiplier.\n",
    "    \"\"\"\n",
    "    num_init: int = 30\n",
    "    recursive_steps: int = 10\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        if self.solver is None:\n",
    "            self.solver = bisection_method(\n",
    "                lambda x, k: self._objective(x, **k)[0],\n",
    "                self.num_init, self.recursive_steps, step_size=0.0\n",
    "            )\n",
    "\n",
    "    def _objective(self, x, **kwargs) -> jax.Array:\n",
    "        return jnp.asarray(self.lagrangian(x, **kwargs))\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_search_bounds(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            log_pi: jax.Array,\n",
    "            *,\n",
    "            inv_beta: jax.Array | None = None,\n",
    "            epsilon: jax.Array | None = None\n",
    "    ) -> tuple[jax.Array, jax.Array]:\n",
    "        \"\"\"Get informative bounds for the normalizer in the true search-space.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def solve(self, *args, **kwargs) -> jxt.ArrayLike:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535822ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixins.py\n",
    "\"\"\"Module for useful mixin classes to extend base class functionalities.\n",
    "\n",
    "\"\"\"\n",
    "from typing import Callable, Any\n",
    "from functools import partial\n",
    "\n",
    "import abc\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "\n",
    "class TypesMixin:\n",
    "    num_init: int\n",
    "    recursive_steps: int\n",
    "    solver: Any\n",
    "    bounds: tuple[float, float]\n",
    "\n",
    "    bounds_slack: tuple[float, float] = (1.0, 0.1)\n",
    "\n",
    "    _validate_args: Callable[[jax.Array, jax.Array, bool], None]\n",
    "    _objective: Callable[..., jax.Array]\n",
    "\n",
    "\n",
    "class SolveNormalizerMixin(TypesMixin):\n",
    "    \"\"\"Mixin class to implement a generic solver for the normalization value.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(x):\n",
    "        return jnp.log(x)\n",
    "\n",
    "    def get_search_bounds(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            log_pi: jax.Array,\n",
    "            *,\n",
    "            inv_beta: jax.Array | None = None,\n",
    "            epsilon: jax.Array | None = None\n",
    "    ) -> tuple[jax.Array, jax.Array]:\n",
    "        \"\"\"Get informative bounds in the true search-space.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def solve_normalizer(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            log_pi: jax.Array,\n",
    "            *,\n",
    "            inv_beta: jax.Array | None = None,\n",
    "            epsilon: jax.Array | None = None\n",
    "    ) -> jax.Array:\n",
    "        \"\"\"Do a recursive grid-search + local hill-climber.\n",
    "\n",
    "        Search is optionally performed (default = log) in a monotonically\n",
    "        transformed search-space. This helps the stability of search,\n",
    "        especially at the boundaries.\n",
    "\n",
    "        This method returns the results in the *transformed* space.\n",
    "        \"\"\"\n",
    "        self._validate_args(inv_beta, epsilon, True)\n",
    "\n",
    "        # Compute bounds to speed up search.\n",
    "        canonical_low, canonical_high = self.get_search_bounds(\n",
    "            q, log_pi, inv_beta=inv_beta, epsilon=epsilon\n",
    "        )\n",
    "\n",
    "        results = self.solver(\n",
    "            (self.transform(canonical_low) - self.bounds_slack[0],\n",
    "             self.transform(canonical_high) + self.bounds_slack[1]),\n",
    "            kwargs=dict(log_pi=log_pi, q=q, inv_beta=inv_beta, epsilon=epsilon)\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "class SolveNormalizerAndTrustRegionMixin(TypesMixin):\n",
    "    # Split up compute-budget for normalization and the trust-region.\n",
    "    num_init_tr: int\n",
    "    recursive_steps_tr: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()  # type: ignore\n",
    "\n",
    "        # Quick hacky way to compose two bisection-search methods.\n",
    "        # self.inv_beta_solver = bisection_method(\n",
    "        #     lambda x, k: self._objective(\n",
    "        #         self.solver(\n",
    "        #             k['bounds'],\n",
    "        #             kwargs=dict(log_inv_beta=x) | {\n",
    "        #                 a: b for a, b in k.items() if a != 'bounds'\n",
    "        #             }\n",
    "        #         ),\n",
    "        #         log_inv_beta=x, **{\n",
    "        #             a: b for a, b in k.items() if a != 'bounds'\n",
    "        #         }\n",
    "        #     )[1],\n",
    "        #     self.num_init_tr, self.recursive_steps_tr, step_size=0.1\n",
    "        # )\n",
    "        self.inv_beta_solver = bisection_method(\n",
    "            self._tr_objective,\n",
    "            self.num_init_tr, self.recursive_steps_tr, step_size=0.1\n",
    "        )\n",
    "\n",
    "    def _tr_objective(self, x: jax.Array, kwargs: dict[str, Any]):\n",
    "        unpack = {a: b for a, b in kwargs.items() if a != 'bounds'}\n",
    "\n",
    "        # For a given log_inv_beta = 'x', normalize the solution\n",
    "        f_eta = self.solver(\n",
    "            kwargs['bounds'],\n",
    "            kwargs=dict(log_inv_beta=x) | unpack\n",
    "        )\n",
    "\n",
    "        # Then return the lagrangian of the solution for 'x'\n",
    "        return self._objective(f_eta, log_inv_beta=x, **unpack)[1]\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(x):\n",
    "        return jnp.log(x)\n",
    "\n",
    "    def get_search_bounds(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            log_pi: jax.Array,\n",
    "            *,\n",
    "            inv_beta: jax.Array | None = None,\n",
    "            epsilon: jax.Array | None = None\n",
    "    ) -> tuple[jax.Array, jax.Array]:\n",
    "        \"\"\"Get informative bounds in the true search-space.\"\"\"\n",
    "        ...\n",
    "\n",
    "    def solve(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            log_pi: jax.Array,\n",
    "            *,\n",
    "            inv_beta: jax.Array | None = None,\n",
    "            epsilon: jax.Array | None = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self._validate_args(inv_beta, epsilon, True)\n",
    "\n",
    "        if inv_beta is None:\n",
    "            low, high = self.get_search_bounds(\n",
    "                q, log_pi, epsilon=epsilon\n",
    "            )\n",
    "            search_eta_low = self.transform(low) - self.bounds_slack[0]\n",
    "            search_eta_high = self.transform(high) + self.bounds_slack[1]\n",
    "\n",
    "            log_inv_beta = self.inv_beta_solver(\n",
    "                self.bounds,  # log_inv_beta bounds\n",
    "                kwargs=dict(\n",
    "                    bounds=(search_eta_low, search_eta_high),  # eta bounds\n",
    "                    q=q, log_pi=log_pi, epsilon=epsilon\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            log_inv_beta = jnp.log(inv_beta)\n",
    "\n",
    "        # Find the normalizer using tight bounds.\n",
    "        low, high = self.get_search_bounds(\n",
    "            q, log_pi, inv_beta=jnp.exp(log_inv_beta)\n",
    "        )\n",
    "        search_eta_low = self.transform(low) - self.bounds_slack[0]\n",
    "        search_eta_high = self.transform(high) + self.bounds_slack[1]\n",
    "\n",
    "        # If inv_beta is given, we can reduce to 1D optimization\n",
    "        search_eta_star = self.solver(\n",
    "            (search_eta_low, search_eta_high),\n",
    "            kwargs=dict(q=q, log_pi=log_pi, log_inv_beta=log_inv_beta)\n",
    "        )\n",
    "        return search_eta_star, log_inv_beta\n",
    "\n",
    "\n",
    "class SandwichTrustRegionMixin(TypesMixin, abc.ABC):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Core Constants\n",
    "    logits: bool\n",
    "    _epsilon_ltol: float\n",
    "    _epsilon_rtol: float\n",
    "\n",
    "    # Core Methods\n",
    "    epsilon_greedy: Callable[[jax.Array, jax.typing.ArrayLike], jax.Array]\n",
    "    trust_region_upperbound: Callable[[jax.Array, jax.Array], jax.Array]\n",
    "\n",
    "    # Mixin-functionality\n",
    "    _greedy_jitter: float = 0.0\n",
    "\n",
    "    def _trust_region_interior(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            pi: jax.Array,\n",
    "            *,\n",
    "            epsilon: jax.typing.ArrayLike | None = None,\n",
    "            inv_beta: jax.typing.ArrayLike | None = None\n",
    "    ) -> jax.Array:\n",
    "        ...\n",
    "\n",
    "    def _trust_region_boundary(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            pi: jax.Array,\n",
    "            *,\n",
    "            use_prior: bool\n",
    "    ):\n",
    "        \"\"\"Get q-star when the trust-region constraint gives an extreme-case\n",
    "\n",
    "        If epsilon is tiny, then return the prior. If epsilon exceeds the\n",
    "        divergence between the greedy policy and the prior, return the greedy\n",
    "        policy.\n",
    "        \"\"\"\n",
    "        greedy = self.epsilon_greedy(q, self._greedy_jitter)\n",
    "        out = jax.lax.select(use_prior, pi, greedy)\n",
    "\n",
    "        # Warning will return -infs for epsilon \\approx 1.0\n",
    "        return jnp.log(out) if self.logits else out\n",
    "\n",
    "    def __call__(\n",
    "            self,\n",
    "            q: jax.Array,\n",
    "            pi: jax.Array,\n",
    "            *,\n",
    "            epsilon: jax.typing.ArrayLike | None = None,\n",
    "            inv_beta: jax.typing.ArrayLike | None = None\n",
    "    ) -> jax.Array:\n",
    "        self._validate_args(inv_beta, epsilon, True)\n",
    "\n",
    "        all_same = jnp.isclose(q, q[0]).all()\n",
    "\n",
    "        if epsilon is not None:\n",
    "            epsilon_ub = self.trust_region_upperbound(q, pi)\n",
    "\n",
    "            use_prior = all_same | (epsilon < self._epsilon_ltol)\n",
    "            use_greedy = epsilon > jnp.clip(epsilon_ub - self._epsilon_rtol, 0)\n",
    "\n",
    "            return jax.lax.cond(\n",
    "                ~use_prior & ~use_greedy,\n",
    "                partial(self._trust_region_interior, epsilon=epsilon),\n",
    "                partial(self._trust_region_boundary, use_prior=use_prior),\n",
    "                q, pi\n",
    "            )\n",
    "\n",
    "        # We cannot check bounds for inv_beta without knowing the normalizer.\n",
    "        inv_beta = jnp.clip(inv_beta, jnp.exp(self.bounds[0]))\n",
    "        return jax.lax.cond(\n",
    "            ~all_same,\n",
    "            partial(self._trust_region_interior, inv_beta=inv_beta),\n",
    "            lambda *_: pi,\n",
    "            q, pi\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state, checkpoints\n",
    "import wandb\n",
    "\n",
    "class Agent: \n",
    "    def __init__(self, params):\n",
    "\n",
    "        self.network = params.get(\"network\", PolicyValueNetwork_2048)(num_actions=params[\"num_actions\"], num_channels=params[\"num_channels\"])\n",
    "        self.optimizer = optax.adam(params['lr'])\n",
    "        self.input_shape = self.input_shape_fn(params[\"obs_spec\"])\n",
    "\n",
    "        self.key = jax.random.PRNGKey(params['seed'])\n",
    "        \n",
    "        self.train_state = train_state.TrainState.create(\n",
    "            apply_fn=self.network.apply,\n",
    "            params=self.network.init(self.key, jnp.ones((1, *self.input_shape))),\n",
    "            tx=self.optimizer\n",
    "        )\n",
    "\n",
    "        self.net_apply_fn = jax.jit(self.train_state.apply_fn)\n",
    "        self.grad_fn = jax.value_and_grad(self.loss_fn)\n",
    "\n",
    "        self.last_mse_losses = []\n",
    "        self.last_kl_losses = []\n",
    "\n",
    "    def input_shape_fn(self, observation_spec):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_state_from_observation(self, observation, batched):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def normalize_rewards(self, r):\n",
    "        return r\n",
    "\n",
    "    def reverse_normalize_rewards(self, r):\n",
    "        return r\n",
    "\n",
    "    def save(self, path, step):\n",
    "        checkpoints.save_checkpoint(\n",
    "            target=self.train_state,\n",
    "            ckpt_dir=path,\n",
    "            step=step,\n",
    "            overwrite=True,\n",
    "            prefix=\"agent_\",\n",
    "        )\n",
    "\n",
    "    def loss_fn(self, params, states, actions, returns, episode):\n",
    "        # KL Loss for policy part of the network:\n",
    "        probs, values = self.net_apply_fn(params, states)\n",
    "        # optax expects this to be log probabilities\n",
    "        log_probs = jnp.log(probs + 1e-9)\n",
    "\n",
    "        targets = actions\n",
    "\n",
    "        kl_loss = optax.losses.kl_divergence(log_predictions=log_probs, targets=targets)\n",
    "        kl_loss = jnp.mean(kl_loss)\n",
    "\n",
    "        # MSE Loss for value part of the network:\n",
    "        mse_loss = optax.l2_loss(values.flatten(), returns)\n",
    "        mse_loss = jnp.mean(mse_loss)\n",
    "\n",
    "        self.last_mse_losses.append(mse_loss.item())\n",
    "        self.last_kl_losses.append(kl_loss.item())\n",
    "\n",
    "        return kl_loss + mse_loss\n",
    "\n",
    "\n",
    "    def update_fn(self, states, actions, returns, episode):\n",
    "        returns = self.normalize_rewards(returns)\n",
    "        loss, grads = self.grad_fn(self.train_state.params, states, actions, returns, episode)\n",
    "        self.train_state = self.train_state.apply_gradients(grads=grads)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def get_output(self, state):\n",
    "        mask = state.action_mask\n",
    "\n",
    "        # the state has to be gotten depending on the environment\n",
    "        state = self.get_state_from_observation(state, True)\n",
    "\n",
    "        # forward pass of the network\n",
    "        actions, value = self.net_apply_fn(self.train_state.params, state)\n",
    "        actions = jnp.ravel(actions)\n",
    "        value = jnp.ravel(value)[0]\n",
    "\n",
    "        # mask and renormalize the actions\n",
    "        masked_actions = self.mask_actions(actions, mask)\n",
    "        renormalized_actions = masked_actions / jnp.sum(masked_actions)\n",
    "\n",
    "        value = self.reverse_normalize_rewards(value)\n",
    "\n",
    "        return renormalized_actions, value\n",
    "\n",
    "    def log_losses(self, episode, params):\n",
    "        if params[\"logging\"]:\n",
    "            wandb.log({\n",
    "                \"kl_loss\": sum(self.last_kl_losses) / len(self.last_kl_losses),\n",
    "                \"mse_loss\": sum(self.last_mse_losses) / len(self.last_mse_losses),\n",
    "\n",
    "            }, step=episode*params[\"num_steps\"]*params[\"num_batches\"])\n",
    "        self.last_kl_losses = []\n",
    "        self.last_mse_losses = []\n",
    "        \n",
    "    \n",
    "    def mask_actions(self, actions, mask):\n",
    "        return jnp.where(mask, actions, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve_norm.py\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import typing as jxt\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExPropKullbackLeibler(\n",
    "    SandwichTrustRegionMixin, SolveNormalizerMixin, NumericalNormalizerObjective\n",
    "):\n",
    "    \"\"\"Implements the Kullback-Leibler divergence from prior to model.\n",
    "\n",
    "    KL(prior || model) = int_X prior(x) log(prior(x) / model(x)) dx\n",
    "\n",
    "    This KL is often obtained in empirical density fitting, as its\n",
    "    minimization w.r.t. the model is equivalent to minimization of the\n",
    "    cross-entropy loss. It is also obtained by swapping the model and\n",
    "    prior arguments for the Variational KL. This is known in literature as\n",
    "    Expectation-Propagation.\n",
    "\n",
    "    This divergence is also referred to, ambiguously, as the forward-KL.\n",
    "\n",
    "    For parametric model-fitting, this divergence induces moment-matching\n",
    "    behaviour.\n",
    "    \"\"\"\n",
    "\n",
    "    num_init: int = 16\n",
    "    recursive_steps: int = 5\n",
    "    bounds: tuple[float, float] = (-20, 10.0)\n",
    "\n",
    "    _greedy_jitter: float = 1e-2\n",
    "\n",
    "    def lagrangian(\n",
    "        self,\n",
    "        log_eta: jax.Array,\n",
    "        q: jax.Array,\n",
    "        log_pi: jax.Array,\n",
    "        *,\n",
    "        inv_beta: jax.Array | None = None,\n",
    "        epsilon: jax.Array | None = None,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Computes the partial log-Lagrangian for the normalizer eta.\"\"\"\n",
    "        self._validate_args(inv_beta, epsilon, True)\n",
    "\n",
    "        log_z = jax.nn.logsumexp(\n",
    "            jnp.asarray([jnp.broadcast_to(log_eta, q.shape), jnp.log(-q + 1e-32)]),\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        if epsilon is not None:\n",
    "            log_inv_beta = jnp.sum(jnp.exp(log_pi) * log_z) - epsilon\n",
    "        else:\n",
    "            log_inv_beta = jnp.log(inv_beta)\n",
    "\n",
    "        log_q_star = log_pi + log_inv_beta - log_z\n",
    "\n",
    "        # 1 - exp(logits) = 0 --> logits = 0\n",
    "        return (jax.nn.logsumexp(log_q_star),)\n",
    "\n",
    "    def get_search_bounds(\n",
    "        self,\n",
    "        q: jax.Array,\n",
    "        log_pi: jax.Array,\n",
    "        *,\n",
    "        inv_beta: jax.Array | None = None,\n",
    "        epsilon: jax.Array | None = None,\n",
    "    ) -> tuple[jax.Array, jax.Array]:\n",
    "        # Bound for eta depends on beta_inv. Given epsilon, the solution to\n",
    "        # beta_inv depends on eta. So unless the constraint is soft, we\n",
    "        # need to expand the search-domain for the hard-constraint program.\n",
    "        if epsilon is not None:\n",
    "            return (\n",
    "                jnp.max(q + jnp.exp(self.bounds[0]) * jnp.exp(log_pi)),\n",
    "                q.max() + jnp.exp(self.bounds[-1]),\n",
    "            )\n",
    "\n",
    "        return jnp.max(q + inv_beta * jnp.exp(log_pi)), q.max() + inv_beta\n",
    "\n",
    "    @staticmethod\n",
    "    def divergence(q_star: jax.Array, pi: jax.Array) -> jxt.ArrayLike:\n",
    "        divergence = pi * (\n",
    "            jnp.clip(jnp.log(pi), -1e3) - jnp.clip(jnp.log(q_star), -1e3)\n",
    "        )\n",
    "        return divergence.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def inv_beta(\n",
    "        q: jax.Array,\n",
    "        pi: jax.Array,\n",
    "        eta: jax.Array,\n",
    "        epsilon: jxt.ArrayLike | None = None,\n",
    "    ) -> jax.Array:\n",
    "        # Get analytical solution to the trust-region multiplier.\n",
    "        log_z = jnp.log(eta - q)\n",
    "        return jnp.exp(jnp.sum(pi * log_z) - epsilon)\n",
    "\n",
    "    def _trust_region_interior(\n",
    "        self,\n",
    "        q: jax.Array,\n",
    "        pi: jax.Array,\n",
    "        *,\n",
    "        epsilon: jxt.ArrayLike | None = None,\n",
    "        inv_beta: jxt.ArrayLike | None = None,\n",
    "    ) -> jax.Array:\n",
    "        q = q - q.max()\n",
    "        log_pi = jnp.log(jnp.clip(pi, 1e-16))\n",
    "\n",
    "        log_eta = self.solve_normalizer(q, log_pi, inv_beta=inv_beta, epsilon=epsilon)\n",
    "\n",
    "        log_z = jax.nn.logsumexp(\n",
    "            jnp.asarray([jnp.broadcast_to(log_eta, q.shape), jnp.log(-q + 1e-32)]),\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        if epsilon is not None:\n",
    "            log_inv_beta = jnp.sum(jnp.exp(log_pi) * log_z) - epsilon\n",
    "        else:\n",
    "            log_inv_beta = jnp.log(inv_beta)\n",
    "\n",
    "        logits = log_pi + log_inv_beta - log_z\n",
    "        return logits if self.logits else jnp.exp(logits)\n",
    "\n",
    "    def trust_region_upperbound(self, q: jax.Array, pi: jax.Array) -> jax.Array:\n",
    "        \"\"\"Uses the KL-divergence from pi to an epsilon-greedy policy.\n",
    "\n",
    "        We have to use an epsilon-greedy policy as a heuristic since the\n",
    "        KL from pi to greedy is undefined. The greedy policy is the dirac\n",
    "        measure on the maximum of q. The logarithm inside the KL grows to\n",
    "        infinity due to this measure.\n",
    "        \"\"\"\n",
    "        max_supported = self.divergence(\n",
    "            self(q, pi, inv_beta=jnp.exp(self.bounds[0])), pi\n",
    "        )\n",
    "        jittered = self.divergence(self.epsilon_greedy(q, self._greedy_jitter), pi)\n",
    "        return jnp.minimum(max_supported, jittered)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SquaredHellinger(\n",
    "    SandwichTrustRegionMixin, SolveNormalizerMixin, NumericalNormalizerObjective\n",
    "):\n",
    "    \"\"\"Implements the squared Hellinger distance from prior to model.\n",
    "\n",
    "    H^2(prior, model) = 2 - 2int_X sqrt(prior(x) * model(x)) dx\n",
    "\n",
    "    This objective is symmetric in its arguments and bounded between [0, 1].\n",
    "\n",
    "    Note: as a result of the divergence being bounded, epsilon and inv_beta\n",
    "          are also bounded. If epsilon exceeds tuned bounds, we opt for a\n",
    "          uniform or greedy distribution to prevent numerical problems in\n",
    "          the solver. For inv_beta we cannot check the bounds without knowing\n",
    "          the normalizer, so we clip it to the epsilon-bound in the solver.\n",
    "    \"\"\"\n",
    "\n",
    "    num_init: int = 16\n",
    "    recursive_steps: int = 5\n",
    "    bounds: tuple[float, float] = (-20, 10.0)\n",
    "\n",
    "    _epsilon_rtol: float = 0.01\n",
    "\n",
    "    def lagrangian(\n",
    "        self,\n",
    "        log_eta: jax.Array,\n",
    "        q: jax.Array,\n",
    "        log_pi: jax.Array,\n",
    "        *,\n",
    "        inv_beta: jax.Array | None = None,\n",
    "        epsilon: jax.Array | None = None,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Computes the partial log-Lagrangian for the normalizer eta.\"\"\"\n",
    "        self._validate_args(inv_beta, epsilon, True)\n",
    "        eta = jnp.exp(log_eta)\n",
    "\n",
    "        log_z = jnp.log((2 * jnp.abs(eta - q)) + 1e-32)\n",
    "        norm = jax.nn.logsumexp(log_pi - log_z)\n",
    "\n",
    "        if epsilon is not None:\n",
    "            log_inv_beta = jnp.log(1 - epsilon) - norm\n",
    "        else:\n",
    "            log_inv_beta = jnp.log(jnp.clip(inv_beta, 1e-32))\n",
    "\n",
    "        log_q_star = log_pi + 2 * (log_inv_beta - log_z)\n",
    "\n",
    "        # 1 - exp(logits) = 0 --> logits = 0\n",
    "        return (jax.nn.logsumexp(log_q_star),)\n",
    "\n",
    "    def get_search_bounds(\n",
    "        self,\n",
    "        q: jax.Array,\n",
    "        log_pi: jax.Array,\n",
    "        *,\n",
    "        inv_beta: jax.Array | None = None,\n",
    "        epsilon: jax.Array | None = None,\n",
    "    ) -> tuple[jax.Array, jax.Array]:\n",
    "        # Bound for eta depends on beta_inv. Given epsilon, the solution to\n",
    "        # beta_inv depends on eta. So unless the constraint is soft, we\n",
    "        # need to expand the search-domain for the hard-constraint program.\n",
    "        if epsilon is not None:\n",
    "            return (\n",
    "                jnp.max(q + jnp.exp(self.bounds[0]) * jnp.exp(0.5 * log_pi)),\n",
    "                q.max() + jnp.exp(self.bounds[-1]),\n",
    "            )\n",
    "\n",
    "        return (jnp.max(q + inv_beta * jnp.exp(0.5 * log_pi)), q.max() + inv_beta)\n",
    "\n",
    "    @staticmethod\n",
    "    def inv_beta(\n",
    "        q: jax.Array,\n",
    "        pi: jax.Array,\n",
    "        eta: jax.Array,\n",
    "        epsilon: jxt.ArrayLike | None = None,\n",
    "    ) -> jax.Array:\n",
    "        # Get analytical solution to the trust-region multiplier.\n",
    "        z = (2 * jnp.abs(eta - q)) + 1e-8\n",
    "        norm = jnp.sum(pi / z)\n",
    "\n",
    "        return (1 - epsilon) / norm\n",
    "\n",
    "    @staticmethod\n",
    "    def divergence(q_star: jax.Array, pi: jax.Array) -> jxt.ArrayLike:\n",
    "        return 1 - jnp.sqrt(q_star * pi).sum()\n",
    "\n",
    "    def _trust_region_interior(\n",
    "        self,\n",
    "        q: jax.Array,\n",
    "        pi: jax.Array,\n",
    "        *,\n",
    "        epsilon: jxt.ArrayLike | None = None,\n",
    "        inv_beta: jxt.ArrayLike | None = None,\n",
    "    ) -> jax.Array:\n",
    "        q = q - q.max()\n",
    "        log_pi = jnp.log(jnp.clip(pi, 1e-32))\n",
    "\n",
    "        log_eta = self.solve_normalizer(q, log_pi, inv_beta=inv_beta, epsilon=epsilon)\n",
    "\n",
    "        log_z = jnp.log((2 * jnp.abs(jnp.exp(log_eta) - q)) + 1e-32)\n",
    "        norm = jax.nn.logsumexp(log_pi - log_z)\n",
    "\n",
    "        if epsilon is not None:\n",
    "            log_inv_beta = jnp.log(1 - epsilon) - norm\n",
    "        else:\n",
    "            log_inv_beta = jnp.log(jnp.clip(inv_beta, 1e-32))\n",
    "\n",
    "        log_q_star = log_pi + 2 * (log_inv_beta - log_z)\n",
    "\n",
    "        return log_q_star if self.logits else jnp.exp(log_q_star)\n",
    "\n",
    "    def trust_region_upperbound(self, q: jax.Array, pi: jax.Array) -> jax.Array:\n",
    "        max_supported = self.divergence(\n",
    "            self(q, pi, inv_beta=jnp.exp(self.bounds[0])), pi\n",
    "        )\n",
    "        jittered = self.divergence(self.epsilon_greedy(q, self._greedy_jitter), pi)\n",
    "        return jnp.minimum(max_supported, jittered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb874a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve_trust_region.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import typing as jxt\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VariationalKullbackLeibler(\n",
    "    SandwichTrustRegionMixin, NumericalTrustRegionObjective\n",
    "):\n",
    "    \"\"\"Implements the Kullback-Leibler divergence from model to prior.\n",
    "\n",
    "    KL(model || prior) = int_X model(x) log(model(x) / prior(x)) dx\n",
    "\n",
    "    This is the KL as derived uniquely through the evidence lower bound.\n",
    "    It is also referred to, ambiguously, as the reverse-KL.\n",
    "\n",
    "    For parametric model-fitting, this divergence induces mode-finding\n",
    "    behaviour.\n",
    "    \"\"\"\n",
    "\n",
    "    num_init: int = 10\n",
    "    recursive_steps: int = 5\n",
    "\n",
    "    bounds: tuple[float, float] = (-20.0, 10.0)\n",
    "\n",
    "    _greedy_jitter = 0.01\n",
    "\n",
    "    def lagrangian(\n",
    "        self,\n",
    "        min_log_beta: jax.Array,\n",
    "        q: jax.Array,\n",
    "        log_pi: jax.Array,\n",
    "        *,\n",
    "        epsilon: jax.Array,\n",
    "    ) -> tuple:\n",
    "        \"\"\"Computes the partial log-Lagrangian for inv_beta in log-space.\"\"\"\n",
    "        logits = log_pi + q * jnp.exp(-min_log_beta)\n",
    "        log_z = jax.nn.logsumexp(logits)\n",
    "\n",
    "        log_q_star = logits - log_z\n",
    "        return (jnp.log(epsilon) - jax.nn.logsumexp(log_q_star, b=log_q_star - log_pi),)\n",
    "\n",
    "    def solve_trust_region(\n",
    "        self, q: jax.Array, log_pi: jax.Array, epsilon: jxt.ArrayLike\n",
    "    ) -> jax.Array:\n",
    "        # = log(inv_beta)\n",
    "        return self.solver(\n",
    "            self.bounds, kwargs=dict(q=q, log_pi=log_pi, epsilon=epsilon)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def divergence(q_star: jax.Array, pi: jax.Array) -> jxt.ArrayLike:\n",
    "        divergence = q_star * (\n",
    "            jnp.clip(jnp.log(q_star), -1e3) - jnp.clip(jnp.log(pi), -1e3)\n",
    "        )\n",
    "        return divergence.sum()\n",
    "\n",
    "    def _trust_region_interior(\n",
    "        self,\n",
    "        q: jax.Array,\n",
    "        pi: jax.Array,\n",
    "        *,\n",
    "        epsilon: jxt.ArrayLike | None = None,\n",
    "        inv_beta: jxt.ArrayLike | None = None,\n",
    "    ) -> jax.Array:\n",
    "        q = q - q.max()\n",
    "        log_pi = jnp.log(jnp.clip(pi, 1e-16))\n",
    "\n",
    "        if epsilon is not None:\n",
    "            min_log_beta = self.solve_trust_region(q, log_pi, epsilon=epsilon)\n",
    "            inv_beta = jnp.exp(min_log_beta)\n",
    "\n",
    "        logits = log_pi + q / jnp.clip(inv_beta, 1e-16)\n",
    "\n",
    "        if self.logits:\n",
    "            return logits\n",
    "\n",
    "        return jax.nn.softmax(logits)\n",
    "\n",
    "    def trust_region_upperbound(self, q: jax.Array, pi: jax.Array) -> jax.Array:\n",
    "        max_supported = self.divergence(\n",
    "            self(q, pi, inv_beta=jnp.exp(self.bounds[0])), pi\n",
    "        )\n",
    "        jittered = self.divergence(self.epsilon_greedy(q, self._greedy_jitter), pi)\n",
    "        return jnp.minimum(max_supported, jittered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edaea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_2048.py\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Agent2048(Agent): \n",
    "    def __init__(self, params):\n",
    "        params[\"network\"] = PolicyValueNetwork_2048\n",
    "        super().__init__(params)\n",
    "\n",
    "\n",
    "    def normalize_rewards(self, r):\n",
    "        # Log2 the rewards and then normalize given that 2^16 is the highest realistic reward\n",
    "        # return jnp.log2(r + 1) / 16\n",
    "        return r / 50\n",
    "\n",
    "    def reverse_normalize_rewards(self, r):\n",
    "         # do the reverse of the above\n",
    "        # return 2 ** (r * 16) - 1\n",
    "        return r * 50\n",
    "\n",
    "    def input_shape_fn(self, observation_spec):\n",
    "        return observation_spec.board.shape\n",
    "\n",
    "    def get_state_from_observation(self, observation, batched=True):\n",
    "        state = observation.board\n",
    "        if batched and len(state.shape) == 2:\n",
    "            state = state[None, ...]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0119062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_knapsack.py\n",
    "import jax.numpy as jnp\n",
    "import haiku.experimental.flax as hkflax\n",
    "import haiku as hk\n",
    "import jax\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "class AgentKnapsack(Agent):\n",
    "    def __init__(self, params):\n",
    "        self.network = hkflax.Module(hk.transform(forward_fn))\n",
    "        self.optimizer = optax.adam(params['lr'])\n",
    "        self.input_shape = self.input_shape_fn(params[\"obs_spec\"])\n",
    "\n",
    "        self.key = jax.random.PRNGKey(params['seed'])\n",
    "        \n",
    "        self.train_state = train_state.TrainState.create(\n",
    "            apply_fn=self.network.apply,\n",
    "            params=self.network.init(self.key, jnp.ones((1, *self.input_shape))),\n",
    "            tx=self.optimizer\n",
    "        )\n",
    "\n",
    "        self.net_apply_fn = jax.jit(self.train_state.apply_fn)\n",
    "        self.grad_fn = jax.value_and_grad(self.loss_fn)\n",
    "    \n",
    "        self.last_mse_losses = []\n",
    "        self.last_kl_losses = []\n",
    "    # def mask_actions(self, actions, mask):\n",
    "    #     return actions\n",
    "\n",
    "    def input_shape_fn(self, observation_spec):\n",
    "        return (4, *observation_spec.weights.shape)\n",
    "\n",
    "    def get_state_from_observation(self, observation, batched=True):\n",
    "        state = jnp.stack([observation.weights, observation.values, observation.packed_items, observation.action_mask], axis=-2)\n",
    "        assert state.shape[-2:] == (4, observation.weights.shape[-1])\n",
    "        if batched and len(state.shape) == 2:\n",
    "            state = state[None, ...]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_maze.py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "class AgentMaze(Agent): \n",
    "    def __init__(self, params, env):\n",
    "        params[\"policy_network\"] = CNNPolicyNetwork\n",
    "        params[\"value_network\"] = CNNValueNetwork\n",
    "        # TODO: Think those are not really needed. Check if we can remove it. \n",
    "\n",
    "        self.key = jax.random.PRNGKey(params['seed'])\n",
    "        self.env = env        \n",
    "        state, self.timestep = jax.jit(env.reset)(self.key)\n",
    "        #print(self.timestep.discount)\n",
    "        self._observation_spec = state\n",
    "        self._action_spec = env.action_spec\n",
    "\n",
    "        self.policy_network = params.get(\"policy_network\")(num_actions=self._action_spec.num_values, num_channels=params['num_channels'])\n",
    "        self.value_network = params.get(\"value_network\")(num_channels=params['num_channels'])\n",
    "        self.policy_optimizer = optax.adam(params['lr'])\n",
    "        self.value_optimizer = optax.adam(params['lr'])\n",
    "\n",
    "        self.input_shape = self.input_shape_fn(self._observation_spec).shape\n",
    "        print(self.input_shape)\n",
    "\n",
    "        key1, key2 = jax.random.split(self.key)\n",
    "        \n",
    "        self.policy_train_state = train_state.TrainState.create(\n",
    "            apply_fn=self.policy_network.apply,\n",
    "            params=self.policy_network.init(key1, jnp.ones((1, *self.input_shape))),\n",
    "            tx=self.policy_optimizer\n",
    "        )\n",
    "\n",
    "        self.value_train_state = train_state.TrainState.create(\n",
    "            apply_fn=self.value_network.apply,\n",
    "            params=self.value_network.init(key2, jnp.ones((1, *self.input_shape))),\n",
    "            tx=self.value_optimizer\n",
    "        )\n",
    "\n",
    "        self.policy_apply_fn = jax.jit(self.policy_train_state.apply_fn)\n",
    "        self.value_apply_fn = jax.jit(self.value_train_state.apply_fn)\n",
    "\n",
    "        self.policy_grad_fn = jax.value_and_grad(self.compute_policy_loss)\n",
    "        self.value_grad_fn = jax.value_and_grad(self.compute_value_loss)\n",
    "\n",
    "\n",
    "    def input_shape_fn(self, observation_spec):\n",
    "        return self.process_observation(observation_spec)\n",
    "\n",
    "    def get_state_from_observation(self, observation, batched=True):\n",
    "        state = self.process_observation(observation)\n",
    "        if batched and len(state.shape) == 3:\n",
    "            state = state[None, ...]\n",
    "        return state\n",
    "    \n",
    "    def process_observation(self, observation):\n",
    "        \"\"\"Add the agent and the target to the walls array.\"\"\"\n",
    "        agent = 2\n",
    "        target = 3\n",
    "        obs = observation.walls.astype(float)\n",
    "        obs = obs.at[tuple(observation.agent_position)].set(agent)\n",
    "        obs = obs.at[tuple(observation.target_position)].set(target)\n",
    "        # jax.debug.print(\"{obs}\", obs=obs)\n",
    "        return jnp.expand_dims(obs, axis=-1)  # Adding a channels axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_snake.py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "class AgentSnake(Agent): \n",
    "    def __init__(self, params):\n",
    "        params[\"network\"] = PolicyValueNetwork\n",
    "        super().__init__(params)\n",
    "\n",
    "\n",
    "    def input_shape_fn(self, observation_spec):\n",
    "        return observation_spec.grid.shape\n",
    "\n",
    "    def get_state_from_observation(self, observation, batched=True):\n",
    "        state = observation.grid\n",
    "        if batched and len(state.shape) == 3:\n",
    "            state = state[None, ...]\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebe35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_selection.py\n",
    "import chex\n",
    "import jax\n",
    "import mctx\n",
    "\n",
    "\n",
    "\n",
    "def custom_action_selection(\n",
    "        rng_key: chex.PRNGKey,\n",
    "        tree: mctx.Tree,\n",
    "        node_index: chex.Numeric,\n",
    "        depth: chex.Numeric,\n",
    "        *,\n",
    "        pb_c_init: float = 1.25,\n",
    "        pb_c_base: float = 19652.0,\n",
    "        qtransform=mctx.qtransform_by_parent_and_siblings,\n",
    "        selector=VariationalKullbackLeibler()\n",
    ") -> chex.Array:\n",
    "    \"\"\"Returns the action selected for a node index.\n",
    "\n",
    "    See Appendix B in https://arxiv.org/pdf/1911.08265.pdf for more details.\n",
    "\n",
    "    Args:\n",
    "      rng_key: random number generator state.\n",
    "      tree: _unbatched_ MCTS tree state.\n",
    "      node_index: scalar index of the node from which to select an action.\n",
    "      depth: the scalar depth of the current node. The root has depth zero.\n",
    "      pb_c_init: constant c_1 in the PUCT formula.\n",
    "      pb_c_base: constant c_2 in the PUCT formula.\n",
    "      qtransform: a monotonic transformation to convert the Q-values to [0, 1].\n",
    "\n",
    "    Returns:\n",
    "      action: the action selected from the given node.\n",
    "    \"\"\"\n",
    "    prior_logits = tree.children_prior_logits[node_index]\n",
    "    prior_probs = jax.nn.softmax(prior_logits)\n",
    "    value_score = qtransform(tree, node_index)\n",
    "    dist = selector(prior_logits, value_score, inv_beta=1.0)\n",
    "\n",
    "    # Add tiny bit of randomness for tie break\n",
    "\n",
    "    # Masking the invalid actions at the root.\n",
    "    #   return masked_argmax(to_argmax, tree.root_invalid_actions * (depth == 0))\n",
    "    return jax.random.categorical(rng_key, dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f0a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree_policies.py\n",
    "import functools\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import mctx\n",
    "from mctx import search\n",
    "\n",
    "def muzero_custom_policy(\n",
    "    params,\n",
    "    selector,\n",
    "    rng_key: chex.PRNGKey,\n",
    "    root: mctx.RootFnOutput,\n",
    "    recurrent_fn: mctx.RecurrentFn,\n",
    "    num_simulations: int,\n",
    "    invalid_actions: Optional[chex.Array] = None,\n",
    "    max_depth: Optional[int] = None,\n",
    "    loop_fn = jax.lax.fori_loop,\n",
    "    *,\n",
    "    qtransform = mctx.qtransform_by_parent_and_siblings,\n",
    "    dirichlet_fraction: chex.Numeric = 0.25,\n",
    "    dirichlet_alpha: chex.Numeric = 0.3,\n",
    "    pb_c_init: chex.Numeric = 1.25,\n",
    "    pb_c_base: chex.Numeric = 19652,\n",
    "    temperature: chex.Numeric = 1.0) -> mctx.PolicyOutput[None]:\n",
    "  \"\"\"Runs MuZero search and returns the `PolicyOutput`.\n",
    "\n",
    "  In the shape descriptions, `B` denotes the batch dimension.\n",
    "\n",
    "  Args:\n",
    "    params: params to be forwarded to root and recurrent functions.\n",
    "    rng_key: random number generator state, the key is consumed.\n",
    "    root: a `(prior_logits, value, embedding)` `RootFnOutput`. The\n",
    "      `prior_logits` are from a policy network. The shapes are\n",
    "      `([B, num_actions], [B], [B, ...])`, respectively.\n",
    "    recurrent_fn: a callable to be called on the leaf nodes and unvisited\n",
    "      actions retrieved by the simulation step, which takes as args\n",
    "      `(params, rng_key, action, embedding)` and returns a `RecurrentFnOutput`\n",
    "      and the new state embedding. The `rng_key` argument is consumed.\n",
    "    num_simulations: the number of simulations.\n",
    "    invalid_actions: a mask with invalid actions. Invalid actions\n",
    "      have ones, valid actions have zeros in the mask. Shape `[B, num_actions]`.\n",
    "    max_depth: maximum search tree depth allowed during simulation.\n",
    "    loop_fn: Function used to run the simulations. It may be required to pass\n",
    "      hk.fori_loop if using this function inside a Haiku module.\n",
    "    qtransform: function to obtain completed Q-values for a node.\n",
    "    dirichlet_fraction: float from 0 to 1 interpolating between using only the\n",
    "      prior policy or just the Dirichlet noise.\n",
    "    dirichlet_alpha: concentration parameter to parametrize the Dirichlet\n",
    "      distribution.\n",
    "    pb_c_init: constant c_1 in the PUCT formula.\n",
    "    pb_c_base: constant c_2 in the PUCT formula.\n",
    "    temperature: temperature for acting proportionally to\n",
    "      `visit_counts**(1 / temperature)`.\n",
    "\n",
    "  Returns:\n",
    "    `PolicyOutput` containing the proposed action, action_weights and the used\n",
    "    search tree.\n",
    "  \"\"\"\n",
    "  rng_key, dirichlet_rng_key, search_rng_key = jax.random.split(rng_key, 3)\n",
    "\n",
    "  # Adding Dirichlet noise.\n",
    "  noisy_logits = _get_logits_from_probs(\n",
    "      _add_dirichlet_noise(\n",
    "          dirichlet_rng_key,\n",
    "          jax.nn.softmax(root.prior_logits),\n",
    "          dirichlet_fraction=dirichlet_fraction,\n",
    "          dirichlet_alpha=dirichlet_alpha))\n",
    "  root = root.replace(\n",
    "      prior_logits=_mask_invalid_actions(noisy_logits, invalid_actions))\n",
    "\n",
    "  # Running the search.\n",
    "  interior_action_selection_fn = functools.partial(\n",
    "      custom_action_selection,\n",
    "      selector=selector,\n",
    "    #   pb_c_base=pb_c_base,\n",
    "    #   pb_c_init=pb_c_init,\n",
    "      qtransform=qtransform)\n",
    "  root_action_selection_fn = functools.partial(\n",
    "      interior_action_selection_fn,\n",
    "      depth=0)\n",
    "  search_tree = search(\n",
    "      params=params,\n",
    "      rng_key=search_rng_key,\n",
    "      root=root,\n",
    "      recurrent_fn=recurrent_fn,\n",
    "      root_action_selection_fn=root_action_selection_fn,\n",
    "      interior_action_selection_fn=interior_action_selection_fn,\n",
    "      num_simulations=num_simulations,\n",
    "      max_depth=max_depth,\n",
    "      invalid_actions=invalid_actions,\n",
    "      loop_fn=loop_fn)\n",
    "\n",
    "  # Sampling the proposed action proportionally to the visit counts.\n",
    "  summary = search_tree.summary()\n",
    "  action_weights = summary.visit_probs\n",
    "  action_logits = _apply_temperature(\n",
    "      _get_logits_from_probs(action_weights), temperature)\n",
    "  action = jax.random.categorical(rng_key, action_logits)\n",
    "  return mctx.PolicyOutput(\n",
    "      action=action,\n",
    "      action_weights=action_weights,\n",
    "      search_tree=search_tree)\n",
    "  \n",
    "def _mask_invalid_actions(logits, invalid_actions):\n",
    "  \"\"\"Returns logits with zero mass to invalid actions.\"\"\"\n",
    "  if invalid_actions is None:\n",
    "    return logits\n",
    "  chex.assert_equal_shape([logits, invalid_actions])\n",
    "  logits = logits - jnp.max(logits, axis=-1, keepdims=True)\n",
    "  # At the end of an episode, all actions can be invalid. A softmax would then\n",
    "  # produce NaNs, if using -inf for the logits. We avoid the NaNs by using\n",
    "  # a finite `min_logit` for the invalid actions.\n",
    "  min_logit = jnp.finfo(logits.dtype).min\n",
    "  return jnp.where(invalid_actions, min_logit, logits)\n",
    "\n",
    "\n",
    "def _get_logits_from_probs(probs):\n",
    "  tiny = jnp.finfo(probs).tiny\n",
    "  return jnp.log(jnp.maximum(probs, tiny))\n",
    "\n",
    "\n",
    "def _add_dirichlet_noise(rng_key, probs, *, dirichlet_alpha,\n",
    "                         dirichlet_fraction):\n",
    "  \"\"\"Mixes the probs with Dirichlet noise.\"\"\"\n",
    "  chex.assert_rank(probs, 2)\n",
    "  chex.assert_type([dirichlet_alpha, dirichlet_fraction], float)\n",
    "\n",
    "  batch_size, num_actions = probs.shape\n",
    "  noise = jax.random.dirichlet(\n",
    "      rng_key,\n",
    "      alpha=jnp.full([num_actions], fill_value=dirichlet_alpha),\n",
    "      shape=(batch_size,))\n",
    "  noisy_probs = (1 - dirichlet_fraction) * probs + dirichlet_fraction * noise\n",
    "  return noisy_probs\n",
    "\n",
    "\n",
    "def _apply_temperature(logits, temperature):\n",
    "  \"\"\"Returns `logits / temperature`, supporting also temperature=0.\"\"\"\n",
    "  # The max subtraction prevents +inf after dividing by a small temperature.\n",
    "  logits = logits - jnp.max(logits, keepdims=True, axis=-1)\n",
    "  tiny = jnp.finfo(logits.dtype).tiny\n",
    "  return logits / jnp.maximum(tiny, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c33e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main.py\n",
    "import functools\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "\n",
    "import flashbax as fbx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from flax.training import checkpoints\n",
    "\n",
    "import logging\n",
    "import jumanji\n",
    "import mctx\n",
    "from jumanji.environments.routing.maze import generator\n",
    "from jumanji.types import StepType\n",
    "from jumanji.wrappers import AutoResetWrapper\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Environments: Snake-v1, Knapsack-v1, Game2048-v1, Maze-v0\n",
    "params = {\n",
    "    \"env_name\": \"Knapsack-v1\",\n",
    "    \"maze_size\": (5, 5),\n",
    "    \"policy\": \"KL_ex_prop\",\n",
    "    \"agent\": AgentKnapsack,\n",
    "    \"num_channels\": 32,\n",
    "    \"seed\": 42,\n",
    "    \"lr\": 2e-4,  # 0.00003\n",
    "    \"num_episodes\": 4000,\n",
    "    \"num_steps\": 200,\n",
    "    \"num_actions\": 4,\n",
    "    \"obs_spec\": Optional,\n",
    "    \"buffer_max_length\": 20000,\n",
    "    \"buffer_min_length\": 256,\n",
    "    \"num_batches\": 64,\n",
    "    \"sample_size\": 256,\n",
    "    \"num_simulations\": 8,  # 16,\n",
    "    \"max_tree_depth\": 4,  # 12,\n",
    "    \"discount\": 1,\n",
    "    \"logging\": True,\n",
    "    \"run_in_kaggle\": True,\n",
    "    \"checkpoint_dir\": r'/kaggle/working',\n",
    "    \"checkpoint_interval\": 5,\n",
    "}\n",
    "\n",
    "policy_dict = {\n",
    "    \"default\": mctx.muzero_policy,\n",
    "    \"KL_variational\": functools.partial(\n",
    "        muzero_custom_policy, selector=VariationalKullbackLeibler()\n",
    "    ),\n",
    "    \"KL_ex_prop\": functools.partial(\n",
    "        muzero_custom_policy, selector=ExPropKullbackLeibler()\n",
    "    ),\n",
    "    \"squared_hellinger\": functools.partial(\n",
    "        muzero_custom_policy, selector=SquaredHellinger()\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class Timestep:\n",
    "    \"\"\"Tuple for storing the step type and reward together.\n",
    "    TODO Consider renaming to avoid confusion with the environment timestep.\n",
    "\n",
    "    Attributes:\n",
    "        step_type: The type of the step (e.g., LAST).\n",
    "        reward: The reward received at this timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, step_type, reward):\n",
    "        self.step_type = step_type\n",
    "        self.reward = reward\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def env_step(state, action):\n",
    "    \"\"\"A single step in the environment.\"\"\"\n",
    "    next_state, next_timestep = env.step(state, action)\n",
    "    return next_state, next_timestep\n",
    "\n",
    "\n",
    "def ep_loss_reward(timestep):\n",
    "    \"\"\"Reward transformation for the environment.\"\"\"\n",
    "    new_reward = jnp.where(timestep.step_type == StepType.LAST, -10, timestep.reward)\n",
    "    return new_reward\n",
    "\n",
    "\n",
    "def recurrent_fn(agent: Agent, rng_key, action, embedding):\n",
    "    \"\"\"One simulation step in MCTS.\"\"\"\n",
    "    del rng_key\n",
    "\n",
    "    (state, timestep) = embedding\n",
    "    new_state, new_timestep = env_step(state, action)\n",
    "\n",
    "    # get the action probabilities from the network\n",
    "    prior_logits, value = agent.get_output(new_timestep.observation)\n",
    "\n",
    "    # return the recurrent function output\n",
    "    recurrent_fn_output = mctx.RecurrentFnOutput(\n",
    "        reward=timestep.reward,\n",
    "        discount=params[\"discount\"],\n",
    "        prior_logits=prior_logits,\n",
    "        value=value,\n",
    "    )\n",
    "    return recurrent_fn_output, (new_state, new_timestep)\n",
    "\n",
    "\n",
    "def get_actions(agent, state, timestep, subkey):\n",
    "    \"\"\"Get the actions from the MCTS\"\"\"\n",
    "\n",
    "    def root_fn(state, timestep, _):\n",
    "        \"\"\"Root function for the MCTS.\"\"\"\n",
    "        priors, value = agent.get_output(timestep.observation)\n",
    "\n",
    "        root = mctx.RootFnOutput(\n",
    "            prior_logits=priors,\n",
    "            value=value,\n",
    "            embedding=(state, timestep),\n",
    "        )\n",
    "        return root\n",
    "\n",
    "    policy = policy_dict[params[\"policy\"]]\n",
    "\n",
    "    policy_output = policy(\n",
    "        params=agent,\n",
    "        rng_key=subkey,\n",
    "        root=jax.vmap(root_fn, (None, None, 0))(\n",
    "            state, timestep, jnp.ones(1)\n",
    "        ),  # params[\"num_steps\"])),\n",
    "        recurrent_fn=jax.vmap(recurrent_fn, (None, None, 0, 0)),\n",
    "        num_simulations=params[\"num_simulations\"],\n",
    "        max_depth=params[\"max_tree_depth\"],\n",
    "        # max_num_considered_actions=params[\"num_actions\"],\n",
    "        qtransform=partial(\n",
    "            mctx.qtransform_completed_by_mix_value,\n",
    "            value_scale=0.1,\n",
    "            maxvisit_init=50,\n",
    "            rescale_values=False,\n",
    "        ),\n",
    "        # gumbel_scale=1.0,\n",
    "    )\n",
    "\n",
    "    return policy_output\n",
    "\n",
    "\n",
    "def get_rewards(timestep, prev_reward_arr, episode):\n",
    "    rewards = []\n",
    "    new_reward_arr = []\n",
    "\n",
    "    max_reward = jnp.max(timestep.reward)\n",
    "\n",
    "    # go over all batches\n",
    "    for batch_num in range(len(prev_reward_arr)):\n",
    "\n",
    "        # the previous rewards for this batch\n",
    "        prev_rewards = prev_reward_arr[batch_num]\n",
    "\n",
    "        # go over all timesteps in the batch\n",
    "        for i, (step_type, ep_rew) in enumerate(\n",
    "            zip(timestep.step_type[batch_num], timestep.reward[batch_num])\n",
    "        ):\n",
    "            # if the episode has ended, add the total reward to the rewards list\n",
    "            if step_type == StepType.LAST:\n",
    "                # add the reward from the entire game and the timestep it happened\n",
    "                rew = {\n",
    "                    \"reward\": sum(prev_rewards) + ep_rew,\n",
    "                    \"max_reward\": max_reward,\n",
    "                }\n",
    "                prev_rewards = []\n",
    "\n",
    "                rewards.append(rew)\n",
    "                if params[\"logging\"]:\n",
    "                    wandb.log(\n",
    "                        rew,\n",
    "                        step=(episode - 1) * params[\"num_batches\"] * params[\"num_steps\"]\n",
    "                        + batch_num * params[\"num_steps\"]\n",
    "                        + (i + 1),\n",
    "                    )\n",
    "            else:\n",
    "                prev_rewards.append(ep_rew)\n",
    "\n",
    "        new_reward_arr.append(prev_rewards)\n",
    "\n",
    "    avg_reward = sum([r[\"reward\"] for r in rewards]) / max(1, len(rewards))\n",
    "\n",
    "    steps = (episode - 1) * params[\"num_batches\"] * params[\"num_steps\"] + params[\n",
    "        \"num_steps\"\n",
    "    ]\n",
    "    print(\n",
    "        f\"Episode {episode}, Average reward: {str(round(avg_reward, 1))}, Max Reward: {max_reward}, Steps: {steps} / {params['num_episodes']*params['num_batches']*params['num_steps']}\"\n",
    "    )\n",
    "\n",
    "    return new_reward_arr\n",
    "\n",
    "\n",
    "def step_fn(agent, state_timestep, subkey):\n",
    "    \"\"\"A single step in the environment.\"\"\"\n",
    "    state, timestep = state_timestep\n",
    "    actions = get_actions(agent, state, timestep, subkey)\n",
    "\n",
    "    assert actions.action.shape[0] == 1\n",
    "    assert actions.action_weights.shape[0] == 1\n",
    "\n",
    "    # key = jax.random.PRNGKey(42)\n",
    "    best_action = actions.action[0]\n",
    "\n",
    "    state, timestep = env_step(state, best_action)\n",
    "    q_value = actions.search_tree.summary().qvalues[\n",
    "        actions.search_tree.ROOT_INDEX, best_action\n",
    "    ]\n",
    "    # timestep.extra[\"game_reward\"]\n",
    "\n",
    "    return (state, timestep), (timestep, actions.action_weights[0], q_value)\n",
    "\n",
    "\n",
    "def run_n_steps(state, timestep, subkey, agent, n):\n",
    "    random_keys = jax.random.split(subkey, n)\n",
    "    # partial function to be able to send the agent as an argument\n",
    "    partial_step_fn = functools.partial(step_fn, agent)\n",
    "    # scan over the n steps\n",
    "    (next_ep_state, next_ep_timestep), (cum_timestep, actions, q_values) = jax.lax.scan(\n",
    "        partial_step_fn, (state, timestep), random_keys\n",
    "    )\n",
    "    return cum_timestep, actions, q_values, next_ep_state, next_ep_timestep\n",
    "\n",
    "\n",
    "def gather_data(state, timestep, subkey):\n",
    "    keys = jax.random.split(subkey, params[\"num_batches\"])\n",
    "    timestep, actions, q_values, next_ep_state, next_ep_timestep = jax.vmap(\n",
    "        run_n_steps, in_axes=(0, 0, 0, None, None)\n",
    "    )(state, timestep, keys, agent, params[\"num_steps\"])\n",
    "    # print(timestep.reward.shape)\n",
    "    # print(timestep.step_type.shape)\n",
    "\n",
    "    return timestep, actions, q_values, next_ep_state, next_ep_timestep\n",
    "\n",
    "\n",
    "def train(agent: Agent, action_weights_arr, q_values_arr, states_arr, episode):\n",
    "    losses = [\n",
    "        agent.update_fn(states, actions, q_values, episode)\n",
    "        for actions, q_values, states in zip(\n",
    "            action_weights_arr, q_values_arr, states_arr\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return jnp.mean(jnp.array(losses), axis=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Initialize wandb\n",
    "    if params[\"logging\"]:\n",
    "        init_wandb(params)\n",
    "\n",
    "    # Initialize the environment\n",
    "    if params[\"env_name\"] == \"Maze-v0\":\n",
    "        gen = generator.RandomGenerator(*params[\"maze_size\"])\n",
    "        env = jumanji.make(params[\"env_name\"], generator=gen)\n",
    "    else:\n",
    "        env = jumanji.make(params[\"env_name\"])\n",
    "\n",
    "    print(f\"running {params['env_name']}\")\n",
    "    env = AutoResetWrapper(env)\n",
    "\n",
    "    # Initialize the agent\n",
    "    params[\"num_actions\"] = env.action_spec.num_values\n",
    "    params[\"obs_spec\"] = env.observation_spec\n",
    "    agent = params.get(\"agent\", Agent)(params)\n",
    "\n",
    "    # Specify buffer parameters\n",
    "    buffer = fbx.make_flat_buffer(\n",
    "        max_length=params[\"buffer_max_length\"],\n",
    "        min_length=params[\"buffer_min_length\"],\n",
    "        sample_batch_size=params[\"sample_size\"],\n",
    "        add_batch_size=params[\"num_batches\"],\n",
    "    )\n",
    "\n",
    "    # Jit the buffer functions\n",
    "    buffer = buffer.replace(\n",
    "        init=jax.jit(buffer.init),\n",
    "        add=jax.jit(buffer.add, donate_argnums=0),\n",
    "        sample=jax.jit(buffer.sample),\n",
    "        can_sample=jax.jit(buffer.can_sample),\n",
    "    )\n",
    "\n",
    "    # Specify buffer format\n",
    "    if params[\"env_name\"] in [\"Snake-v1\", \"Knapsack-v1\"]:\n",
    "        fake_timestep = {\n",
    "            \"q_value\": jnp.zeros((params[\"num_steps\"])),\n",
    "            \"actions\": jnp.zeros(\n",
    "                (params[\"num_steps\"], params[\"num_actions\"]), dtype=jnp.float32\n",
    "            ),\n",
    "            \"states\": jnp.zeros(\n",
    "                (params[\"num_steps\"], *agent.input_shape), dtype=jnp.float32\n",
    "            ),\n",
    "        }\n",
    "    else:\n",
    "        fake_timestep = {\n",
    "            \"q_value\": jnp.zeros((params[\"num_steps\"])),\n",
    "            \"actions\": jnp.zeros(\n",
    "                (params[\"num_steps\"], params[\"num_actions\"]), dtype=jnp.float32\n",
    "            ),\n",
    "            \"states\": jnp.zeros(\n",
    "                (params[\"num_steps\"], *agent.input_shape), dtype=jnp.int32\n",
    "            ),\n",
    "        }\n",
    "    buffer_state = buffer.init(fake_timestep)\n",
    "\n",
    "    # Initialize the random keys\n",
    "    key = jax.random.PRNGKey(params[\"seed\"])\n",
    "    rng_key, subkey = jax.random.split(key)\n",
    "    keys = jax.random.split(rng_key, params[\"num_batches\"])\n",
    "\n",
    "    # Get the initial state and timestep\n",
    "    next_ep_state, next_ep_timestep = jax.vmap(env.reset)(keys)\n",
    "\n",
    "    prev_reward_arr = [[] for _ in range(params[\"num_batches\"])]\n",
    "    for episode in range(1, params[\"num_episodes\"] + 1):\n",
    "\n",
    "        # Get new key every episode\n",
    "        key, sample_key = jax.jit(jax.random.split)(key)\n",
    "        # Gather data\n",
    "        timestep, actions, q_values, next_ep_state, next_ep_timestep = gather_data(\n",
    "            next_ep_state, next_ep_timestep, sample_key\n",
    "        )\n",
    "\n",
    "        prev_reward_arr = get_rewards(timestep, prev_reward_arr, episode)\n",
    "\n",
    "        # Get state in the correct format given environment\n",
    "        states = agent.get_state_from_observation(timestep.observation, True)\n",
    "\n",
    "        # Add data to buffer\n",
    "        buffer_state = buffer.add(\n",
    "            buffer_state,\n",
    "            {\n",
    "                \"q_value\": q_values,\n",
    "                \"actions\": actions,\n",
    "                \"states\": states,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        if buffer.can_sample(buffer_state):\n",
    "            key, sample_key = jax.jit(jax.random.split)(key)\n",
    "            data = buffer.sample(buffer_state, sample_key).experience.first\n",
    "            loss = train(\n",
    "                agent, data[\"actions\"], data[\"q_value\"], data[\"states\"], episode\n",
    "            )\n",
    "            agent.log_losses(episode, params)\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        # if params[\"logging\"]:\n",
    "        #     log_rewards(rewards, loss, episode, params)\n",
    "\n",
    "        if episode % params[\"checkpoint_interval\"] == 0:\n",
    "            print(f\"Saving checkpoint for episode {episode}\")\n",
    "            agent.save(params[\"checkpoint_dir\"], episode)\n",
    "\n",
    "    if params[\"logging\"]:\n",
    "        wandb.finish()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
